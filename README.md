# ğŸ§  Sales Copilot Lakehouse
[![Built with Python](https://img.shields.io/badge/Built%20with-Python-blue?logo=python)](https://www.python.org/)
[![Powered by Spark](https://img.shields.io/badge/Data%20Migration-Spark-orange?logo=apache-spark)](https://spark.apache.org/)
[![dbt Models](https://img.shields.io/badge/Data%20Modeling-dbt-red?logo=dbt)](https://www.getdbt.com/)
[![Streamlit App](https://img.shields.io/badge/UI-Streamlit-lightgrey?logo=streamlit)](https://streamlit.io/)
[![RAG LLM](https://img.shields.io/badge/LLM-RAG-green?logo=openai)]()

A full-stack AI assistant built for querying EV product knowledge, powered by a robust Data Engineering pipeline and Retrieval-Augmented Generation (RAG).

> âœ… Designed as a **portfolio project for Data Engineer roles**, integrating batch processing, modeling, BI reporting, and AI-based interaction.

---

## ğŸ“¸ Project Overview

> ğŸš— Use case: Empower sales teams to interact with EV product & sales data via natural language.

![Project Architecture Diagram](./images/architecture.png) <!-- Add your image later -->

---

## ğŸ§° Tech Stack

| Layer             | Tools / Frameworks                                           |
|------------------|--------------------------------------------------------------|
| **Data Ingestion** | Apache Spark (PySpark) + MinIO (S3 API)                      |
| **Transformation**| dbt (data modeling, pipeline orchestration)                  |
| **Data Warehouse**| DuckDB (local), or replaceable with Redshift/Snowflake       |
| **BI Dashboard**  | Apache Superset (sales insights, color/model analysis)       |
| **Embedding Store**| Chroma (local vector DB)                                    |
| **LLM Backend**   | Ollama + Mistral + nomic-embed-text                          |
| **UI Layer**      | Streamlit chatbot                                            |
| **Ops / Dev Tooling** | Ngrok, Python, dotenv, logging                           |

---

## ğŸ“Š Data Pipeline Walkthrough

### 1. ğŸ” Data Migration with Spark

- Used **PySpark** to extract, clean, and load raw sales data into a lakehouse.
- Integrated with **MinIO** as a low-cost S3 alternative for object storage.

```bash
# Sample code snippet
df = spark.read.csv("s3a://lakehouse/sales.csv", header=True)
df.write.parquet("s3a://lakehouse/processed/")
```
ğŸ“· Screenshot: Spark Job in Terminal (TO ADD)

### 2. ğŸ§± Data Modeling with dbt
	â€¢	Created models for products, sales_summary, and color_distribution.
	â€¢	Scheduled and tested transformations locally.
```sql
-- Example dbt model
SELECT 
    model,
    color,
    COUNT(*) AS sales_count
FROM {{ ref('sales') }}
GROUP BY 1, 2

```
ğŸ“· Screenshot: dbt DAG and dbt run logs (TO ADD)

### 3. ğŸ“ˆ Dashboard with Apache Superset
	â€¢	Built an interactive dashboard to analyze:
	â€¢	Most sold EV colors
	â€¢	Region-wise model distribution
	â€¢	Temporal sales trends

ğŸ“· Screenshot: Superset Dashboard (TO ADD)

### 4. ğŸ” Embedding + Vector Search (Chroma)
	â€¢	Loaded all markdown & product PDFs
	â€¢	Used nomic-embed-text for document embeddings via Ollama
	â€¢	Stored vector chunks in a local Chroma DB

ğŸ“· Screenshot: Chroma DB files created (TO ADD)
ğŸ§  Example Query

â€œWhat colors is the AeroFlow model available in?â€

Chatbot Response:
The AeroFlow model is available in Red, Glacier Blue, and Pearl White.

With sources retrieved from embedded markdown/PDF documentation.

##  Architecture Summary
```mermaid
flowchart TD
    A[Raw CSV / Docs] --> B[Apache Spark]
    B --> C[MinIO / S3]
    C --> D[dbt Modeling]
    D --> E[DuckDB / Warehouse]
    E --> F[Superset Dashboards]
    C --> G[LangChain + Embedding]
    G --> H[Chroma Vector Store]
    H --> I[Ollama (LLM)]
    I --> J[Streamlit Chatbot]
```
## ğŸ¯ Skills Demonstrated
	â€¢	âœ… PySpark for large-scale transformation
	â€¢	âœ… dbt for data modeling & CI-friendly pipelines
	â€¢	âœ… Superset for rapid BI development
	â€¢	âœ… LLM apps using LangChain + Ollama + Chroma
	â€¢	âœ… Streamlit for frontend chatbot UI
	â€¢	âœ… Environment control with .env, pyenv, ngrok

## ğŸ“ Next Steps
	â€¢	Add PostgreSQL or Snowflake as production DWH
	â€¢	Deploy chatbot publicly with Docker + Ngrok
	â€¢	Add session memory and multi-turn RAG
## ğŸ“ Repository Structure
<pre><code>Sales-Copilot-Lakehouse/
â”œâ”€â”€ rag/                    # LLM, RAG, Chatbot code
â”‚   â”œâ”€â”€ manage_chroma_db.py
â”‚   â”œâ”€â”€ streamlit_chat.py
â”œâ”€â”€ dbt/                    # dbt models and config
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ dbt_project.yml
â”œâ”€â”€ spark_jobs/            # PySpark scripts for ingestion
â”œâ”€â”€ superset/              # Superset dashboard exports
â”œâ”€â”€ chroma/                # Persisted vector DB
â”œâ”€â”€ images/                # ğŸ“· Screenshot placeholders
â”œâ”€â”€ .env                   # Environment secrets
â””â”€â”€ README.md              # You're here
</code></pre>

