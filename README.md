# ğŸ› ï¸ Production-Ready Data Engineering Pipeline with Generative AI Integration
[![Built with Python](https://img.shields.io/badge/Built%20with-Python-blue?logo=python)](https://www.python.org/)
[![Powered by Spark](https://img.shields.io/badge/Data%20Migration-Spark-orange?logo=apache-spark)](https://spark.apache.org/)
[![dbt Models](https://img.shields.io/badge/Data%20Modeling-dbt-red?logo=dbt)](https://www.getdbt.com/)
[![Dockerized](https://img.shields.io/badge/Deployment-Docker-blue?logo=docker)](https://www.docker.com/)
[![Dagster](https://img.shields.io/badge/Orchestration-Dagster-6E40C9?logo=dagster)](https://dagster.io/)
[![Jupyter Notebooks](https://img.shields.io/badge/Analysis-Jupyter-orange?logo=jupyter)](https://jupyter.org/)
[![Superset](https://img.shields.io/badge/Dashboard-Superset-darkgreen?logo=apache-superset)](https://superset.apache.org/)
[![Streamlit App](https://img.shields.io/badge/UI-Streamlit-lightgrey?logo=streamlit)](https://streamlit.io/)
[![RAG LLM](https://img.shields.io/badge/LLM-RAG-green?logo=openai)]()

A production-grade Data Engineering pipeline that integrates Spark for data migration, dbt for transformation, and Dagster for orchestration. The processed data powers an analytics dashboard in Apache Superset and a Generative AI interface via RAG + LLM using Streamlit. Designed to simulate an EV product data workflow in a modern lakehouse architecture.

> âœ… Designed as a **portfolio project for Data Engineer roles**, integrating batch processing, modeling, BI reporting, and AI-based interaction.

## ğŸ“¸ Project Overview

> ğŸš— **Use case:** Help sales & operations teams explore EV product configurations, color options, and sales trends via a conversational interface.

This project delivers a **production-grade data engineering pipeline** designed for real-time analytics and AI-driven exploration of electric vehicle (EV) sales data. It integrates modern data tooling and large language models (LLMs) to create a seamless, interactive experience for business users.

### ğŸ”§ Key Highlights

- ğŸš€ **Data Ingestion** using PySpark and MinIO (S3-compatible object store)
- ğŸ§± **Data Transformation & Modeling** with dbt, orchestrated by Dagster
- ğŸ” **Query Layer** powered by **Dremio** (SQL over data lake, no warehouse needed)
- ğŸ“Š **BI Layer** built using **Apache Superset** for sales dashboards
- ğŸ§  **Retrieval-Augmented Generation (RAG)** using **ChromaDB** + **Ollama (Mistral)** for smart QA
- ğŸ’¬ **Interactive UI** via **Streamlit** chatbot
- ğŸ³ **Fully containerized** with Docker and served securely via Ngrok

### â“ Example Chat Queries

- "What EV color sold best in Q1 2024?"
- "Show me the sales breakdown for the AeroFlow model by region"
- "List all available models with more than 500km range"

---

## ğŸ—ºï¸ Architecture Diagram

ğŸ“Œ _You can replace the placeholder below with your architecture image. Recommended contents:_

- **Spark ingestion from CSV/JSON into MinIO**
- **dbt DAGs running on Dagster**
- **Dremio as semantic + acceleration layer**
- **Superset for analytics**
- **ChromaDB for vector storage**
- **LLM with Ollama (Mistral)**
- **Chatbot interface via Streamlit**

```text
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   MinIO    â”‚
             â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
            â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
            â”‚  Spark   â”‚
            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   dbt + Dagster â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”
           â”Œâ”€â”€â”‚Dremio â”‚â”€â”€â”
           â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
           â–¼             â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Superset â”‚   â”‚ ChromaDB   â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼              â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Streamlit Chat UI  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§° Tech Stack

| Layer                | Tools & Frameworks                                                                 |
|---------------------|-------------------------------------------------------------------------------------|
| **Data Ingestion**   | [Apache Spark (PySpark)](https://spark.apache.org/) + [MinIO](https://min.io/) (S3 API) |
| **Data Transformation** | [dbt](https://www.getdbt.com/) (data modeling), [Dagster](https://dagster.io/) (orchestration) |
| **Data Warehouse**   | [DuckDB](https://duckdb.org/) (local OLAP engine) â€” pluggable with Redshift / Snowflake |
| **Data Visualization** | [Apache Superset](https://superset.apache.org/) (interactive dashboards & reporting) |
| **Notebook Interface** | [Jupyter](https://jupyter.org/) (development & debugging support)                     |
| **LLM Embedding Store** | [Chroma](https://www.trychroma.com/) (local vector database)                        |
| **LLM Backend**      | [Ollama](https://ollama.com/) + [Mistral](https://mistral.ai/) + [nomic-embed-text](https://huggingface.co/nomic-ai) |
| **Frontend / UI**    | [Streamlit](https://streamlit.io/) chatbot interface (user Q&A with RAG pipeline) |
| **Dev & Ops Tools**  | [Docker](https://www.docker.com/), [Ngrok](https://ngrok.com/), Python, `dotenv`, logging |

---

> ğŸ’¡ *All components are modular and containerized â€” easily portable for deployment or extension.*

## ğŸ“Š Data Pipeline Walkthrough

### 1. ğŸ” Data Migration with Spark

- Used **PySpark** to extract, clean, and load raw sales data into a lakehouse.
- Integrated with **MinIO** as a low-cost S3 alternative for object storage.

```bash
# Sample code snippet
df = spark.read.csv("s3a://lakehouse/sales.csv", header=True)
df.write.parquet("s3a://lakehouse/processed/")
```


### 2. ğŸ§± Data Modeling with dbt
	â€¢	Created models for products, sales_summary, and color_distribution.
	â€¢	Scheduled and tested transformations locally.
```sql
-- Example dbt model
SELECT 
    model,
    color,
    COUNT(*) AS sales_count
FROM {{ ref('sales') }}
GROUP BY 1, 2

```
![ğŸ“· Screenshot: dbt DAG and dbt run logs](screenshots/Global_Asset_Lineage.svg)

### 3. ğŸ“ˆ Dashboard with Apache Superset
	â€¢	Built an interactive dashboard to analyze:
	â€¢	Most sold EV colors
	â€¢	Region-wise model distribution
	â€¢	Temporal sales trends

![ğŸ“· Screenshot: Superset Dashboard](screenshots/dashboard.png)

### 4. ğŸ” Embedding + Vector Search (Chroma)
	â€¢	Loaded all markdown & product PDFs
	â€¢	Used nomic-embed-text for document embeddings via Ollama
	â€¢	Stored vector chunks in a local Chroma DB

ğŸ“· Screenshot: Chroma DB files created (TO ADD)
ğŸ§  Example Query

â€œWhat colors is the AeroFlow model available in?â€

Chatbot Response:
The AeroFlow model is available in Red, Glacier Blue, and Pearl White.

With sources retrieved from embedded markdown/PDF documentation.

##  Architecture Summary
```mermaid
flowchart TD
    A[Raw CSV / Docs] --> B[Apache Spark]
    B --> C[MinIO / S3]
    C --> D[dbt Modeling]
    D --> E[DuckDB / Warehouse]
    E --> F[Superset Dashboards]
    C --> G[LangChain + Embedding]
    G --> H[Chroma Vector Store]
    H --> I[Ollama (LLM)]
    I --> J[Streamlit Chatbot]
```
## ğŸ¯ Skills Demonstrated
	â€¢	âœ… PySpark for large-scale transformation
	â€¢	âœ… dbt for data modeling & CI-friendly pipelines
	â€¢	âœ… Superset for rapid BI development
	â€¢	âœ… LLM apps using LangChain + Ollama + Chroma
	â€¢	âœ… Streamlit for frontend chatbot UI
	â€¢	âœ… Environment control with .env, pyenv, ngrok

## ğŸ“ Next Steps
	â€¢	Add PostgreSQL or Snowflake as production DWH
	â€¢	Deploy chatbot publicly with Docker + Ngrok
	â€¢	Add session memory and multi-turn RAG
## ğŸ“ Repository Structure
<pre><code>Sales-Copilot-Lakehouse/
â”œâ”€â”€ rag/                    # LLM, RAG, Chatbot code
â”‚   â”œâ”€â”€ manage_chroma_db.py
â”‚   â”œâ”€â”€ streamlit_chat.py
â”œâ”€â”€ dbt/                    # dbt models and config
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ dbt_project.yml
â”œâ”€â”€ spark_jobs/            # PySpark scripts for ingestion
â”œâ”€â”€ superset/              # Superset dashboard exports
â”œâ”€â”€ chroma/                # Persisted vector DB
â”œâ”€â”€ images/                # ğŸ“· Screenshot placeholders
â”œâ”€â”€ .env                   # Environment secrets
â””â”€â”€ README.md              # You're here
</code></pre>

